# 分散システムデザインパターン

# 第一部: シングルノードパターン
関心事を分離するために、仮にシングルノードであったとしても、コンテナを分離させるメリットがある

## 2章: サイドカー
サイドカーパターンは１台のマシン上で動く２つのコンテナから構成される
1. アプリケーションコンテナ
    - アプリケーションのコアロジック
1. サイドカーコンテナ
    - アプリケーションを拡張したり改善したりする
    - ディスクやネットワークなどのリソースをアプリケーションコンテナと共有する

### 2.1 サイドカーの例: レガシーサビスのHTTPS対応
1. レガシーサービスをlocalhostからしかアクセスできないようにする  
1. nginxのサイドカーコンテナを配置する
    - nginxが外部IPからのHTTPSトラフィックを受け取りレガシーサービスに受け流す

### 2.4 gitワークフローを利用したサイドカー
- アプリケーションコンテナ
    - Webサーバが実装されたNode.jsサーバ
- サイドカーコンテナ
    - アプリケーションコンテナとファイルシステムを共有し、ファイルシステムをGitリポジトリと同期するためのループを実行

新しいコードがGitリポジトリにPushされるたびに自動的にロードされる仕組みが簡単にできる

### 2.5 モジュール化と再利用性を考えたサイドカーの設計
#### 2.5.1 パラメータ化されたコンテナ
例えば2.1のnginxのサイドカーの場合、少なくともSSLを利用するための証明書のパスとローカルホストで動いているレガシーアプリケーションのポート番号の２つが必須
- 環境変数か、コマンドライン引数で渡す必要がある
- パラメータ化することで、コンテナは実行するたびに呼び出される関数のように扱える

### 2.6 まとめ
- サイドカーパターンではサイドカーコンテナがアプリケーションコンテナを増強し、拡張する
- サイドカーは、アプリケーションに変更を加えるにはコストが高すぎる際にレガシーアプリケーションを更新するために利用できる

## 3章: アンバサダ
アンバサダコンテナがアプリケーションコンテナとそれ以外の間のやり取りを仲介する仕組み
- モジュール化されて再利用可能なコンテナを作れる
- 色々なアプリケーションコンテナと組み合わせが可能

### 3.1 サービスのシャーディングへのアンバサダの利用
一つのバックエンドストレージしか想定していない既存コードにシャーディングクライアントを組み込むのは難しい
- アンバサダコンテナにシャーディングのロジックを持つことで関心ごとの分離ができる

### 3.3 システムの実験的運用やリクエスト分割への利用
- アプリケーション側に細工をせずに10%のリクエストだけ実験用のシステムに送る

## 4章: アダプタ
- 他のアプリリプケーションが期待する定義済みのインターフェイスのルールを守ったままアプリケーションコンテナのインターフェイスを変える

### 4.1 監視
- アダプタコンテナがアプリケーションコンテナが公開している監視インターフェイスを汎用的な監視システムが使うインターフェイスに変換する
- 監視アダプタを別コンテナとしてデプロイすると、CPUやメモリのようなリソースが別々で割り当てられるので監視アダプタが不正な動きをしてもユーザー向けのサービスに影響を及ぼさないようにできる

### 4.2 ロギング
- fluentdをアダプタコンテナとしてredisのslow-queryをデバッグする

# 第二部: マルチノードパターン
## 5章: レプリカがロードバランスされたサービス
### 5.1.1 ロードバランスのためのReadiness Probe
- アプリケーションがユーザーからのリクエストに応答する準備が整っているかを判断
  - コンテナが起動していても準備ができているとは限らない
- チェック用のURLを作っておく

### 5.2 セッションを保存するサービス
- キャッシュヒット率をあげる等の目的で同じユーザーは同じレプリカに送りたいケースを想定
  - IPアドレスのハッシュを取り、どのレプリカに送るかを決める
  - (IPベースのセッション保存はクラスタ内であれば有効だが、外部IPを利用する場合はNATのために正常に動作しない可能性がある)

### 5.4 キャッシュレイヤの導入
- サイドカーとしてキャッシュコンテナをアプリケーションコンテナと同じPod内に配置するのは筋が良くない
  - キャッシュコンテナをアプリケーションと一緒にスケールする必要があるから
  - キャッシュの保存量とヒット率を考えるとキャッシュコンテナの数は少ないほうがいい

## 6章: シャーディングされたサービス
- シャーディングされたサービス: リクエストの一部に対してのみ応答できる
  - レプリカ: ステートレスなサービス
  - シャーディング: ステートフルなサービス

### 6.1 シャーディングされたキャッシュ
シャーディングされたキャッシュをデザインするときには以下の注意事項がある
- シャーディングされたキャッシュの必要性
  - レプリカだと非常に効率が悪い
- アーキテクチャ内でのキャッシュの役割
  - キャッシュが壊れた場合、ユーザーに対してどんな影響が出るか
    - レプリカならほぼ問題がないし、水平方向の拡張は容易
    - シャーディングだとシャードが復活するまで特定のユーザーに対してキャッシュミスが発生し続ける
  - シャーディングされたキャッシュをアップグレードしたりデプロイし直したりすると新しいレプリカがデプロイされるだけではなく、負荷も上がる
    - 一時的にある程度のキャパシティを失うことになるから
- シャーディングされたキャッシュのレプリカ
  - 以下の場合にシャーディングされたキャッシュのレプリカを作ることが有効
    - レイテンシや負荷の点でキャッシュへの依存が大きい
    - 特定のキャッシュシャードへの負荷が非常に高く、負荷を下げるためにスケールの必要がある

### 6.2 シャーディング関数
- コンシステントハッシュ関数
  - 例えばシャードを10 -> 11にするのは処理上は簡単だが、キャッシュミス率が上がってしまう問題がある
  - コンシステントハッシュ関数はキー数を変更後のシャード数で割った数だけをマッピングし直すように保証された特殊なハッシュ関数
    - コンシステントハッシュ関数を使えばシャード数を10 -> 11に増やしてもマッピングし直されるキーは10%未満(キー数/11)になる

### 6.4 ホットシャーディングシステム
ユーザーの口コミ等で特定のシャードに負荷がかかる場合(ホットシャード)、シャードにレプリカがあれば負荷に応じてキャッシュシャードをスケールできる
- キャッシュシャードにオートスケールを設定していれば、サービスへのトラフィックのパターンに応じて各シャードを動的に大きくしたり小さくしたりできる

## 7章: スキャッタ・ギャザー
- リクエストの並列化で処理を高速化できる
  - 各レプリカが処理の一部を担当し、ルートサーバは全部分を受け取って完全なレスポンスを返す
  - 1台のマシン上のコア数やメモリ、ネットワーク、ディスク帯域幅が複数のマシンに分散されるので、CPU以外はボトルネックにならない

### 7.2 リーフをシャーディングしたスキャッタ・ギャザー
全世界の特許を検索する仕組みを作る場合
- データは複数のノードにシャーディングされる
- 検索の際は全てのノードで検索が走る
- ルートノードが1つにまとめてレスポンスをする

#### 適切なリーフノード数の決め方
並列度を増やせばいいわけではない
- 並列度を増やすとそれだけオーバーヘッドも発生する
- 「落ちこぼれ問題」が存在する
  - 全てのリーフからのレスポンスを待つ必要があるが一台遅いノードがあるだけで全体が引きづられる

各シャードにレプリカを持つように作らなければならない

## 8章: ファンクションとイベント駆動処理
### 8.1 FaaSを使うべきときの判断
利点
- コードと実際に可動するサービスの距離が劇的に短い
- 自動的にスケールする
- マシン障害で失敗したら他のマシンで自動的に再実行される
- ステートレスなので自然にモジュール化される

課題
- 全ての状態データはストレージサービスに保存する必要がある
- 運用が難しくなる
  - 全体像の把握
  - 問題の切り分け

バックグラウンド処理の必要性があるか
- ファンクションを実行するインスタンスの実行時間には制約があるのが普通なので優先度が低く、時間がかかる計算処理には向いていない
  - 動画変換
  - ログファイルの圧縮

### 8.2 FaaSのパターン
#### デコレータパターン
リクエストまたはレスポンスを変換する役割
- 2章のアダプタパターンでも代用できそうだが、その場合はAPIサービス自体と一緒にスケールすることになるが、Functionでやりたいのは軽量な処理の場合が多いので無駄が発生する

Kubelessを使うとkubectlコマンドでfunctionsをデプロイできる

## 9章: オーナーシップの選出
役割の割当をスケールさせる方法

### マスタ選出の必要性の判断
Kubernetes上でシングルトンで動かしている場合以下のメリットがある
- コンテナがクラッシュしたら、そのコンテナは自動的に再起動される
- ヘルスチェックが設定されている状態でコンテナがハングアップしたら、そのコンテナは自動的に再起動される
- マシンがダウンしたら、コンテナは他のマシンへ移動される

が、どの程度ダウンタイムが許されるサービスか次第では、これでは問題になることもある  
その場合はサービスで複数のレプリカを動かし、そのうちの一つのレプリカのみが指定されたオーナーになるという仕組みにする必要がある

### マスタ選出の基本
etcd/ZooKeeper/Consulなどの分散ストレージがマスタ選出のコンセンサスアルゴリズムを実装しおり、以下の機能を提供してくれる
- レプリカを使った信頼性の高いデータストア
- 複雑なロック
- 選出の中層層を作るのに必要な基礎的な部分

# 第三部 バッチ処理パターン
## 10章: ワークキューシステム
再利用可能なコンテナを使ったワークキューを作るには汎用ライブラリコンテナとユーザーが定義するアプリケーションロジックの間のインターフェースが定義されている必要がある  

コンテナ化されたワークキューには、以下の２つのインターフェースが必要
- 処理されるべきワークアイテムを生成するソースコンテナのインターフェース
  - アンバサダパターンで実装可能
  - HTTP RESTful APIが一般的
- 実際にワークアイテムを処理するワーカーコンテナのインターフェース
  - 基本的に一度APIを叩かれる以外の入力はなく、セキュリティの懸念もあるためファイルベースのAPIを使う
